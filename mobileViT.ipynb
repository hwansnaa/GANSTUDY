{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3mVdVOdlQsXJhmRx+OitH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwansnaa/GANSTUDY/blob/main/mobileViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiDZ5gF5gMtr",
        "outputId": "285ea2f5-e6ad-4af9-f88e-53617cbc2c6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 341 kB/s \n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X5Ggz00GfuBA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    \n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim = -1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        ) if project_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h = self.heads), qkv)\n",
        "\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b p h n d -> b p n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "  \n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
        "            ]))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x"
      ],
      "metadata": {
        "id": "bNHSkF87gXQZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(inp * expansion)\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        if expansion == 1:\n",
        "            self.conv = nn.Sequential(\n",
        "                # dw\n",
        "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
        "                nn.BatchNorm2d(inp),\n",
        "                nn.SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                nn.SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)"
      ],
      "metadata": {
        "id": "RBksQOFnf6mL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.ph, self.pw = patch_size\n",
        "\n",
        "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
        "        self.conv2 = conv_1x1_bn(channel, dim)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
        "\n",
        "        self.conv3 = conv_1x1_bn(dim, channel)\n",
        "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "\n",
        "        # Local representations\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        # Global representations\n",
        "        _, _, h, w = x.shape\n",
        "        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
        "        x = self.transformer(x)\n",
        "        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h//self.ph, w=w//self.pw, ph=self.ph, pw=self.pw)\n",
        "\n",
        "        # Fusion\n",
        "        x = self.conv3(x)\n",
        "        x = torch.cat((x, y), 1)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileViT(nn.Module):\n",
        "    def __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
        "        super().__init__()\n",
        "        ih, iw = image_size\n",
        "        ph, pw = patch_size\n",
        "        assert ih % ph == 0 and iw % pw == 0\n",
        "\n",
        "        L = [2, 4, 3]\n",
        "\n",
        "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
        "\n",
        "        self.mv2 = nn.ModuleList([])\n",
        "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
        "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
        "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))   # Repeat\n",
        "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
        "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
        "        \n",
        "        self.mvit = nn.ModuleList([])\n",
        "        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0]*2)))\n",
        "        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1]*4)))\n",
        "        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2]*4)))\n",
        "\n",
        "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
        "\n",
        "        self.pool = nn.AvgPool2d(ih//32, 1)\n",
        "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.mv2[0](x)\n",
        "\n",
        "        x = self.mv2[1](x)\n",
        "        x = self.mv2[2](x)\n",
        "        x = self.mv2[3](x)      # Repeat\n",
        "\n",
        "        x = self.mv2[4](x)\n",
        "        \n",
        "        x = self.mvit[0](x)\n",
        "\n",
        "        x = self.mv2[5](x)\n",
        "        x = self.mvit[1](x)\n",
        "\n",
        "        x = self.mv2[6](x)\n",
        "        x = self.mvit[2](x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = self.pool(x).view(-1, x.shape[1])\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "vUS611XNgxnr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mobilevit_s():\n",
        "    dims = [144, 192, 240]\n",
        "    channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]\n",
        "    return MobileViT((256, 256), dims, channels, num_classes=1000)\n",
        "\n",
        "def mobilevit_xxs():\n",
        "    dims = [64, 80, 96]\n",
        "    channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320]\n",
        "    return MobileViT((256, 256), dims, channels, num_classes=1000, expansion=2)\n",
        "\n",
        "\n",
        "def mobilevit_xs():\n",
        "    dims = [96, 120, 144]\n",
        "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n",
        "    return MobileViT((256, 256), dims, channels, num_classes=1000)"
      ],
      "metadata": {
        "id": "c9v2m49Hf88S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "9wtprJwof6yJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = torch.randn(5, 3, 256, 256)"
      ],
      "metadata": {
        "id": "CueMjHoqf5t0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = mobilevit_s()\n",
        "out = vit(img)\n",
        "print(out.shape)\n",
        "print(count_parameters(vit))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRMqtGx-kOST",
        "outputId": "3769383b-b197-410f-9f4d-c9c6660b1095"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 1000])\n",
            "5636720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxAnsRMPkPNz",
        "outputId": "59bbf62d-b0cb-42e8-a2e9-692fbe8a3de6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileViT(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): SiLU()\n",
              "  )\n",
              "  (mv2): ModuleList(\n",
              "    (0): MV2Block(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU()\n",
              "        (6): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): MV2Block(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU()\n",
              "        (6): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): MV2Block(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU()\n",
              "        (6): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): MV2Block(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU()\n",
              "        (6): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): MV2Block(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
              "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU()\n",
              "        (6): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): MV2Block(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
              "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU()\n",
              "        (6): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): MV2Block(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
              "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): SiLU()\n",
              "        (6): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (mvit): ModuleList(\n",
              "    (0): MobileViTBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (transformer): Transformer(\n",
              "        (layers): ModuleList(\n",
              "          (0): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=144, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=144, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=144, out_features=288, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=288, out_features=144, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=144, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=144, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=144, out_features=288, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=288, out_features=144, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv3): Sequential(\n",
              "        (0): Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (conv4): Sequential(\n",
              "        (0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "    )\n",
              "    (1): MobileViTBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (transformer): Transformer(\n",
              "        (layers): ModuleList(\n",
              "          (0): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=192, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=192, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=768, out_features=192, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=192, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=192, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=768, out_features=192, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (2): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=192, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=192, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=768, out_features=192, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (3): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=192, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=192, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=768, out_features=192, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv3): Sequential(\n",
              "        (0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (conv4): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "    )\n",
              "    (2): MobileViTBlock(\n",
              "      (conv1): Sequential(\n",
              "        (0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (conv2): Sequential(\n",
              "        (0): Conv2d(160, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (transformer): Transformer(\n",
              "        (layers): ModuleList(\n",
              "          (0): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=240, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=240, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=240, out_features=960, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=960, out_features=240, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=240, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=240, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=240, out_features=960, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=960, out_features=240, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (2): ModuleList(\n",
              "            (0): PreNorm(\n",
              "              (norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): Attention(\n",
              "                (attend): Softmax(dim=-1)\n",
              "                (to_qkv): Linear(in_features=240, out_features=96, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=32, out_features=240, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): PreNorm(\n",
              "              (norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
              "              (fn): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): Linear(in_features=240, out_features=960, bias=True)\n",
              "                  (1): SiLU()\n",
              "                  (2): Dropout(p=0.0, inplace=False)\n",
              "                  (3): Linear(in_features=960, out_features=240, bias=True)\n",
              "                  (4): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (conv3): Sequential(\n",
              "        (0): Conv2d(240, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "      (conv4): Sequential(\n",
              "        (0): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): SiLU()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): SiLU()\n",
              "  )\n",
              "  (pool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "  (fc): Linear(in_features=640, out_features=1000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PetwnkmfkSxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}